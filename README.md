# CarND-Behaviour-Cloning

## Introduction
This is third project for the Udacity Nanodegree on Self-driving Car (CarND).
The purpose is to teach a (simulated) car to drive on an empty track and
maintain itself in the middle of the track.
A convolutional neural network was employed, taking a photo from
the front camera of the car as input and outputing the desired steering angle.

My implementation follows closely this [nVidia paper](http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf).

## Data collection, normalization and augmentation
### Data collection
A sample set of data was provided by Udacity, presumbly having been generated by having a human controlling
the simulated car driving around the track.
Additional data could be collected by similar method (simulated driving)
and could potentially improve the performance, but due to time constraint, I restricted
myself to the abovementioned set of `8036` samples, and make use of data augmentation 
to generate additional dataon the fly during training.

The original set of data consists of 8036 samples, 4361 of which having steering angles
of zero, 1775 with negative steering angles and 1900 with positive steering angles.
Regarding high-curvature tracks, there are 464 samples with steering angle less than -0.2
rad (-11 degree), and 395 with steering angles more than 0.2 rad (11 degree).
The data is clearly unbalanced, and high-curvature cases are under-represented. 
See the histogram plotted below for full details of the original data.

I will address this unbalance and under-representation with data augmentation discussed below.

### Data normalization
The images, originally in RGB, were converted to YUV color space, and the three color 
channels were then recentered and rescaled to the min-max values of `-0.5` and `0.5`.

One third of the top and one fifth of the bottom of the each image were cropped away 
as they contained no relevant information about the position of the car or the track.

Lastly, the images were resized to 200x66 to reduce the dimensionality of the network.
200x66 was the input size chosen in the nVidia paper.

### Data spliting
As I employed an apdaptive optimizer for training (Adam), there were few hyper-parameters 
to tune. In fact, I left all the hyper-paramters at their default/typical values and did
no substatial tuning at all (except for the trail and error of different netword designs
and dimensionality). Thus, the data were only splitted into training set and test set, with
25% of the original data reversed for testing.

### Data augmentation
Additional data were generated using samples from the training sets with the following 
transformation: random horizontal flip, random brightness adjustment, random vertical
and horizontal translation, and the use of photos from the left and right cameras.

1. Flip: The image was flip horizontally and the sign of steering angle was reversed
2. Additional camera: randomly choose from the 3 available camera; if the left camera
was used, the steering angle would be adjusted up by `0.25`, and `-0.25` if the right
camera was used
3. Horizontal shift: to mimic small shift in the position of the car on the track;
the steering angle also need to be adjusted by adding `0.04` per pixel of translation
to the right and subtracting when shifting to the left.
4. Vertical shift: to mimic going up and down slopes
5. Brightness adjustment: to mimic different lighting conditions

This is an image from the orignal data set  
![src](https://raw.githubusercontent.com/9mat/CarND-Behaviour-Cloning/master/img/src.png)

This is the same image after being cropped and resized

![cropped](https://raw.githubusercontent.com/9mat/CarND-Behaviour-Cloning/master/img/cropped.png)

This is several images generated from the same image with random transformation
![jitter](https://raw.githubusercontent.com/9mat/CarND-Behaviour-Cloning/master/img/jitter.png)

The dataset is rather unbalanced, with a large portion of its having steering angle 
of zeros (drivin straight).
In each epochs, 20000 images will be generated for training.
To facilitate learning, at the begining of the training, I decreased the proportion of 
samples with small/zero steering angle in the training set to help the model learn
to drive in difficult cases (curved track).
In later epoches, I would gradually introduce those samples back to make sure the model
also learn to drive straight when needed.
Specifically, in epoch i, I threw away samples with steering angles less than 0.1 rad
with a probability of 1/(1+i). This means that, in the very first epoch, there will be no
small steering angles; while in the 9 epoch, the proportion of small angle samples will be 
90% of that of the original data (plus images from the 2 additonal cameras)

![histogram](https://raw.githubusercontent.com/9mat/CarND-Behaviour-Cloning/master/img/hist.png)

## Neural network design
The design of the final model follow closely the implementation of the nVidia paper.
It includes 5 convolutional layers followed by 4 fully connected layers.
ReLU activation was used throughout.
I also added 3 dropout layers as well as L2 regularization to all appropiate layers to 
prevent over-fitting.

              layer output size
     -------------- ------------------
         InputLayer (None, 66, 200, 3) 
             Lambda (None, 66, 200, 3) 
      Convolution2D (None, 31, 98, 24) 
               Relu (None, 31, 98, 24) 
      Convolution2D (None, 14, 47, 36) 
               Relu (None, 14, 47, 36) 
      Convolution2D (None, 5, 22, 48)  
            Dropout (None, 5, 22, 48)  
               Relu (None, 5, 22, 48)  
      Convolution2D (None, 3, 20, 64)  
               Relu (None, 3, 20, 64)  
      Convolution2D (None, 1, 18, 64)  
            Dropout (None, 1, 18, 64)  
               Relu (None, 1, 18, 64)  
            Flatten (None, 1152)       
              Dense (None, 1024)       
            Dropout (None, 1024)       
               Relu (None, 1024)       
              Dense (None, 128)        
               Relu (None, 128)        
              Dense (None, 64)         
               Relu (None, 64)         
              Dense (None, 16)         
               Relu (None, 16)         
              Dense (None, 1)    
              
## Training
![mse](https://raw.githubusercontent.com/9mat/CarND-Behaviour-Cloning/master/img/mse.png)

The model was trained with Adam optimizer and batch size of 32. 
Data augmentation was used to generate 20000 samples per epoch.
The MSE during training is plotted in the figure above.
Probably due to data augmentation that increases input variance in
training set, the training mean square error was much higher than
the validation set.
The final training took 10 epochs, as additional epochs did not seem
to improve performance.

## Results
[![Driving in Autonomous mode](https://img.youtube.com/vi/CjaymivJ_QU/0.jpg)](https://www.youtube.com/watch?v=CjaymivJ_QU&feature=youtu.be)

